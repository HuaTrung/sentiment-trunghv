{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "from data_helper import * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,y_train,x_test,y_test=load_data_and_labels(\"./data/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import data_helper\n",
    "from text_cnn import TextCNN\n",
    "# from tensorflow.python.compiler import tensorrt as trt\n",
    "import csv\n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras import regularizers\n",
    "from keras.layers import Dense, Input, GlobalMaxPooling1D\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Embedding, Conv2D, MaxPooling2D, Dropout,concatenate\n",
    "from keras.layers.core import Reshape, Flatten\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Model\n",
    "from keras import regularizers\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "from gensim.models import Word2Vec\n",
    "from distutils.version import LooseVersion, StrictVersion\n",
    "import gensim\n",
    "\n",
    "model =  './data/baomoi.window2.vn.model.bin'\n",
    "\n",
    "if os.path.isfile(model):\n",
    "    print(\"Loading word2vec model ...\")\n",
    "if LooseVersion(gensim.__version__) >= LooseVersion(\"1.0.1\"):\n",
    "    word2vec_model = KeyedVectors.load_word2vec_format(model, binary=True)\n",
    "else:\n",
    "    print(\"wrong\")\n",
    "    word2vec_model = Word2Vec.load_word2vec_format(model, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Relevance:\n",
    "    def __init__(self, word2vec):\n",
    "        self.w2v_model = word2vec\n",
    "\n",
    "    def vectorize(self, doc: str) -> np.ndarray:\n",
    "        doc = doc.lower()\n",
    "        words = [w for w in doc.split(\" \")]\n",
    "        word_vecs = []\n",
    "        for word in words:\n",
    "            try:\n",
    "                vec = self.w2v_model[word]\n",
    "                word_vecs.append(vec)\n",
    "            except KeyError:\n",
    "                pass\n",
    "        vector = np.mean(word_vecs, axis=0)\n",
    "        return vector\n",
    "\n",
    "    def _cosine_sim(self, vecA, vecB):\n",
    "        \"\"\"Find the cosine similarity distance between two vectors.\"\"\"\n",
    "        relevance = np.dot(vecA, vecB) / (np.linalg.norm(vecA) * np.linalg.norm(vecB))\n",
    "        if np.isnan(np.sum(relevance)):\n",
    "            return 0\n",
    "        return relevance\n",
    "\n",
    "    def calculate_similarity(self, source_doc, target_docs=None):\n",
    "        if not target_docs:\n",
    "            return []\n",
    "\n",
    "        if isinstance(target_docs, str):\n",
    "            target_docs = [target_docs]\n",
    "\n",
    "        source_vec = self.vectorize(source_doc)\n",
    "        print(source_vec)\n",
    "        results = []\n",
    "        for doc in target_docs:\n",
    "            print(doc)\n",
    "            target_vec = self.vectorize(doc)\n",
    "            sim_score = self._cosine_sim(source_vec, target_vec)\n",
    "            results.append({\"score\": sim_score, \"doc\": doc})\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_str(target):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for all datasets.\n",
    "    \"\"\"\n",
    "    target = re.sub(r\"\\'s\", \" \\'s\", target.lower())\n",
    "    target = re.sub(r\"\\'ve\", \" \\'ve\", target)\n",
    "    target = re.sub(r\"n\\'t\", \" n\\'t\", target)\n",
    "    target = re.sub(r\"\\'re\", \" \\'re\", target)\n",
    "    target = re.sub(r\"\\'d\", \" \\'d\", target)\n",
    "    target = re.sub(r\"\\'ll\", \" \\'ll\", target)\n",
    "    target = re.sub(r\",\", \" , \", target)\n",
    "    target = re.sub(r\"!\", \" ! \", target)\n",
    "    target = re.sub(r\"\\(\", \" \\( \", target)\n",
    "    target = re.sub(r\"\\)\", \" \\) \", target)\n",
    "    target = re.sub(r\"\\?\", \" \\? \", target)\n",
    "    target = re.sub(r\"‚Äî\", \" \", target)\n",
    "    target = re.sub(r\"\\s{2,}\", \" \", target)\n",
    "    target = re.sub(r\"\\s{2,}\", \" \", target)\n",
    "    target = re.sub(r\"xem\", \" \", target)\n",
    "    target = re.sub(r\"th√™m\", \" \", target)\n",
    "    target = re.sub(r\"watch\", \" \", target)\n",
    "    target = re.sub(r\"more\", \" \", target)\n",
    "    target= re.sub(\"^\\d+\\s|\\s\\d+\\s|\\s\\d+$\", \" \", target)\n",
    "    target = re.sub(r'[^\\w]', ' ', target)\n",
    "    return target.translate(str.maketrans('', '', string.punctuation)).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = Relevance(word2vec_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.calculate_similarity(post,data[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'score': 0.40205836, 'doc': 'Mua c√°i n√†y ƒëc cc g√¨ v·∫≠y ae ?'}]\n",
      "[{'score': 0.36237624, 'doc': 'pr m·ª•c d·ªØ ha ai ch·∫≥ng c√≥ 2 con m·∫Øt'}]\n",
      "[{'score': 0.4259047, 'doc': 'Gi·∫£m ƒë·ªô kh√≥ neeko ƒëi ad ∆°i =(((( n√¢ng c·∫•p th·∫ø n√†y l√¢u vl =(('}]\n",
      "[{'score': 0.53901136, 'doc': 'C≈©ng th√≠ch th·∫ßn h·ªèa l·∫Øm m√† h·ªïng c√≥ ti·ªÅn ._.'}]\n",
      "[{'score': 0.46181348, 'doc': 'Tui mua 2 b·ªô sao v√†o game n√≥ hi·ªán l√™n b·ªô c∆° b·∫£n v·∫≠y m·ªçi ng∆∞·ªùi'}]\n",
      "[{'score': 0.22009137, 'doc': 'Th·∫ø cho ti·ªÅn mua ƒëi :))'}]\n",
      "[{'score': 0.35390085, 'doc': 'Ai ch∆∞a c√≥ mua ƒëi·ªÉm danh'}]\n",
      "[{'score': 0.3756716, 'doc': 'Mua tr·ªçn h·∫øt th·∫ßn h·ªèa c√°c t∆∞·ªõng'}]\n",
      "[{'score': 0.42413175, 'doc': 'üî• Chi ti·∫øt v·ªÅ Th·∫ßn H·ªèa: http://bit.ly/ThanHoa\\nüî• C√¢u h·ªèi th∆∞·ªùng g·∫∑p: http://bit.ly/ThanHoaFAQ\\nüî• Tra c·ª©u Th·∫ßn Ho·∫£: https://thanhoa.lienminh.garena.vn/tra-cuu/'}]\n",
      "[{'score': 0.16912325, 'doc': 'Mua g√≥i s·∫Ω l·ªùi h∆°n =))'}]\n",
      "[{'score': 0.45565122, 'doc': 'Ph·∫£i chi m√¨nh mua ƒë∆∞·ª£c trong l√∫c ch·ªçn t∆∞·ªõng tr∆∞·ªõc khi v√¥ tr·∫≠n'}]\n",
      "[{'score': 0.21600328, 'doc': 'K ai mua ƒë√¢u :)'}]\n",
      "[{'score': 0.7535879, 'doc': 'n·∫øu m√¨nh ch·ªâ mua b·ªô kh·ªüi ƒë·∫ßu v√† c√†y c√°c m·ªëc th√¨ m√¨nh c√≥ ƒë∆∞·ª£c n√¢ng c·∫•p bi·ªÉu c·∫£m th√¥ng th·∫°o kh√¥ng v·∫≠y??'}]\n",
      "[{'score': 0.37641507, 'doc': 'B·ªè ti·ªÅn ra ƒë·ªÉ thu√™ ng∆∞·ªùi ƒë·∫øm xem m√¨nh ƒë√£ ƒÉn ƒë∆∞·ª£c bao nhi√™u b√°t c∆°m, u·ªëng bao nhi√™u c·ªëc n∆∞·ªõc...th·∫≠t ba ch·∫•m ( tr√≠ch XTV )'}]\n",
      "[{'score': 0.5000255, 'doc': 'Ad cho minh h·ªèi. Phi√™n ban m·ªõi nh·∫•t m√¨nh cap nh·∫°t ve m√°y r√≤i. C√†i xong sao v√†o tran choi ko ƒëc. Man h√¨nh c·ª© ƒë√≤i k·∫øt n·ªëi l·∫°i l√† sao'}]\n",
      "[{'score': 0.5072489, 'doc': 'Mua th·∫ßn ho·∫£ c√≥ t√°c d·ª•ng v·∫≠y c√°c b√°c , em t·ªëi c·ªï l·∫Øm hay mua d√πng ƒë·ªÉ khoe'}]\n",
      "[{'score': 0.5361084, 'doc': 'C√≥ c√°ch n√†o kh√¥ng t·ªën rp kh√¥ng :v ch·ª© c√°ch mua th√¨ bi·∫øt d·ªìi ƒë√≥'}]\n",
      "[{'score': 0.32651845, 'doc': 'T√≠nh mua nh∆∞ng em tui ƒë·ªïi m·∫π mk laptop :))'}]\n",
      "[{'score': 0.18654868, 'doc': 'ƒë·ªÉ ti·ªÅn mua kh·∫©u trang ƒëi'}]\n",
      "[{'score': 0.44611132, 'doc': 'm·ªçi ng∆∞·ªùi cho m√¨nh h·ªèi ng·ªçc nh√°nh √°p ƒë·∫£o ƒëang b·ªã l·ªói ph·∫£i k m√¨nh k d√πng ƒë∆∞·ª£c ng·ªçc ·ªü nh√°nh n√†y'}]\n",
      "[{'score': 0.19389537, 'doc': 'Minh T√¢n'}]\n",
      "[{'score': 0.3297643, 'doc': 'V l√† sao ad'}]\n",
      "[{'score': 0.30589497, 'doc': 'Nh·∫£m nh√≠ v√† t·ªën rp'}]\n",
      "[{'score': 0.3640852, 'doc': '1 t∆∞·ªõng 2k5 thl m√† h∆°n 100 t∆∞·ªõng c√≥ h∆°n 14k :>'}]\n",
      "[{'score': 0.4965937, 'doc': 'Ae cho h·ªèi l√∫c n·∫°p th·∫ª bthg ch·ªâ c√≥ m√£ n·∫°p sao gi·ªù c√≥ th√™m c√°i s·ªë th·∫ª garena l√† g√¨ nh·ªâ?'}]\n",
      "[{'score': 0.23917978, 'doc': 'ƒë·∫Øt nh∆∞ qu·ª∑'}]\n",
      "[{'score': 0.19685557, 'doc': 'Th·∫ßn c√°i ƒë·ªãt m·∫π m tr·∫£ h·ªô c√°i khung c√°i'}]\n",
      "[{'score': 0.42178652, 'doc': 'Cho m h·ªèi mua c√°i n√†y t√°c d·ª•ng j k.t·ªën tinh hoa'}]\n",
      "[]\n",
      "[{'score': 0.47693452, 'doc': 'ad n√™n b√°n combo th·∫ßn h·ªèa theo v·ªã tr√≠ s·∫Ω c√≥ nhi·ªÅu ng∆∞·ªùi mua h∆°n ƒë√≥ ch·ª© b√°n combo full t∆∞·ªõng th·∫ø n√†y th√¨ gi√° h∆°i ch√°t qu√°'}]\n",
      "[{'score': 0.35975686, 'doc': 'L√†m v·ªÅ lux ƒëi ad'}]\n",
      "[{'score': 0.45082927, 'doc': 'C√°i ƒë·∫ßu ti√™n ph·∫£i l√† \"tao ƒëang trong alpha\"'}]\n",
      "[{'score': 0.5556274, 'doc': 'Th·∫ßn h·ªèa ch·ªâ ƒë·ªÉ khoe v·∫≠y th√¥i ch·ª© c√≥ tƒÉng s·ª©c m·∫°nh g√¨ trong game ko c√°c bro'}]\n",
      "[{'score': 0.41991407, 'doc': 'C√°i n√†y c√≥ ·∫£nh h∆∞·ªüng t·ªõi ch·ªâ s·ªë trong tr·∫≠n rank k ae'}]\n",
      "[{'score': 0.33229122, 'doc': '._. Sao con n√†y m·∫•y nƒÉm r·ªìi ko b√°n ƒëa s·∫Øc m√πa ƒë√¥ng v·∫≠y'}]\n",
      "[]\n",
      "[{'score': 0.6871941, 'doc': 'Mn cho m√¨nh h·ªèi l√† m√¨nh mua th·∫ßn ho·∫£ rp trc xong mua th·∫ßn ho·∫£ tinh hoa lam l√† ƒëc b·ªô 1 v√† b·ªô kh·ªüi ƒë·∫ßu th√¨ c√≥ 2 b·ªô th√¨ n√≥ hi·ªán b·ªô n√†o v·∫≠y'}]\n",
      "[{'score': 0.33249238, 'doc': '·∫¢nh tuy·ªát k·ªπ alpha sai k√¨a'}]\n",
      "[{'score': 0.47646546, 'doc': 'AD cho em h·ªèi\\nL√†m sao ƒë·ªÉ n√≥ hi·ªán Th·∫ßn H·ªèa B·ªô 1 ·ªü h√†ng ch·ªù ·∫°.'}]\n",
      "[{'score': 0.4236541, 'doc': 'C√≥ pyke k ad'}]\n",
      "[{'score': 0.5020625, 'doc': 'C√°i n√†y ·ªü ch·ªó ƒë√©o n√†o m√† l·ª•c m√£i ƒë√©o ra ƒë·ªÉ l√†m l·∫°i üôÇ'}]\n",
      "[{'score': 0.42413175, 'doc': 'üî• Chi ti·∫øt v·ªÅ Th·∫ßn H·ªèa: http://bit.ly/ThanHoa\\nüî• C√¢u h·ªèi th∆∞·ªùng g·∫∑p: http://bit.ly/ThanHoaFAQ\\nüî• Tra c·ª©u Th·∫ßn Ho·∫£: https://thanhoa.lienminh.garena.vn/tra-cuu/'}]\n",
      "[{'score': 0.28212884, 'doc': 'Q r·ªìi ch√©m ch√©m, nghe gi·ªëng m√πi l∆∞·ªõt r·ªìi ch·ªçc ch·ªçc üòÇ üòÇ'}]\n",
      "[{'score': 0.09054673, 'doc': 'nh√† ngh√®o hic'}]\n",
      "[{'score': 0.2524428, 'doc': 'L√†m xayah ƒëi ad'}]\n",
      "[{'score': 0.124778055, 'doc': 'L√†m ys ƒë√™'}]\n",
      "[{'score': 0.46993497, 'doc': 'm·∫•y nƒÉm tr∆∞·ªõc xem ƒë∆∞·ª£c th√†nh t·ª±u m√† gi·ªù ph·∫£i mua m·ªõi dc :)))'}]\n",
      "[{'score': 0.32502896, 'doc': 'Nguy·ªÖn Cao ƒê·∫≥ng Pig b·ªô s∆∞u t·∫≠p, ch·ªçn t∆∞·ªõng'}]\n",
      "[{'score': 0.37328386, 'doc': 'Nguy·ªÖn Na C·ª•c c∆∞ng s·∫Ω c√≥ nh√©!'}]\n",
      "[{'score': 0.49041897, 'doc': 'Nguy·ªÖn VƒÉn L·ª£i S·∫Ω c√≥ nha!'}]\n",
      "[{'score': 0.21825248, 'doc': 'Minh Nh√≠ oh v·∫≠y h·∫£ c·∫£m ∆°n qu√™ nh√© üôÇ'}]\n",
      "[{'score': 0.21198379, 'doc': 'M·ªü r·ªìi ƒëi·ªÉm danh ƒë√™ ae ∆°iiii :)))'}]\n",
      "[{'score': 0.18804319, 'doc': 'Nguy·ªÖn Cao ƒê·∫≥ng Pig auke ƒë·ªìng ch√≠'}]\n",
      "[{'score': 0.36540383, 'doc': 'L√†m v·ªÅ Jax ƒëi'}]\n",
      "[{'score': 0.08444257, 'doc': 'Ngon ƒë·∫•y\\nMai mua'}]\n",
      "[{'score': 0.5553552, 'doc': 'c√°i n√†y c√≥ t√°c d·ª•ng g√¨ aer'}]\n",
      "[{'score': 0.31919226, 'doc': 'Trong khi ƒë√≥ t v·∫´n k·∫πt l·∫°i kh√∫c \"I was in alpha'}]\n",
      "[{'score': 0.120019265, 'doc': 'Fizz ƒëi ad'}]\n",
      "[{'score': 0.2769512, 'doc': 'Nguy·ªÖn Cao ƒê·∫≥ng Pig ƒë√¢y bro ∆°i'}]\n",
      "[{'score': 0.22743843, 'doc': '·∫¢nh tuy·ªát kƒ© h∆°i toang nh·ªâ'}]\n",
      "[{'score': 0.3385441, 'doc': 'Li√™n Minh Huy·ªÅn Tho·∫°i ok c·∫£m li√™n minh ht nh√© üôÇ'}]\n",
      "[{'score': 0.50978625, 'doc': 'Quoc Anhh B·∫°n tick v√†o √¥ng sao s√°ng b√™n ph·∫£i t√™n Th·∫ßn Ho·∫£ ƒë·ªÉ ch·ªçn nha!'}]\n",
      "[{'score': 0.4692143, 'doc': 'Cowsep th√≠ch ƒëi·ªÅu n√†y v√† t√¥i c≈©ng v·∫≠y.'}]\n",
      "[{'score': 0.5427937, 'doc': 'cho em h·ªèi ngu c√°i t√°c d·ª•ng c·ªßa c√°i n√†y l√† g√¨ v·∫≠y'}]\n",
      "[{'score': 0.5375227, 'doc': 'Hai Nguyen tu·ª≥ ch·ªânh ƒë∆∞·ª£c nh√©'}]\n",
      "[{'score': 0.6039709, 'doc': 'Hai Nguyen b·ªô n√†o c√≥ ch·ªâ s·ªë cao h∆°n th√¨ n√≥ hi·ªán b·ªô ƒë√≥ th√¥i b·∫°n'}]\n",
      "[{'score': 0.4678115, 'doc': 'T t∆∞·ªüng ƒë·∫Øt l√†m 1 c·ªß v√†o n√†o ng·ªù c√≥ 500k'}]\n",
      "[{'score': 0.22250094, 'doc': 'Trung Nguy·ªÖn thank b·∫°n'}]\n",
      "[{'score': 0.3321689, 'doc': 'M·ªõi s·∫Øp th√¥i'}]\n",
      "[{'score': 0.29944536, 'doc': 'L√†m l·∫°i ·∫£nh chi√™u Q √†'}]\n",
      "[{'score': 0.45595592, 'doc': 'c√°i n√†y ƒë·ªÉ l√†m con m·∫π g√¨ nh·ªâ???'}]\n",
      "[{'score': 0.45413503, 'doc': 'team d√πng bug thi·ªÅn v√† reset ƒë√≤n ƒë√°nh b·∫±ng thi·ªÅn ko th√≠ch ƒëi·ªÅu n√†y :)))'}]\n",
      "[{'score': 0.15687622, 'doc': 'l≈© ch√≥ djt garena n√†y'}]\n",
      "[{'score': 0.43149978, 'doc': 'Ai cho mih xin acc ng·ªçc r·ªìng online vs, ai cho mih xin c·∫£m ∆°n r·∫•t nhi·ªÅu ·∫°, ai k cho ƒë·ª´ng ch·ª≠i mih ·∫°, mih b·ªã m·∫•t njk'}]\n",
      "[{'score': 0.2585375, 'doc': 'Yi 1tr3 tt mua ko ta'}]\n",
      "[{'score': 0.55844253, 'doc': 'Game l√† ƒë·ªÉ gi·∫£i tr√≠ tu·∫ßn ƒëi l√†m 6 ng√†y c√≥ m·ªói ng√†y ch·ªß nh·∫≠t th∆∞ gi·∫£n t√Ω m√† l·∫°i th·∫ßn v·ªõi h·ªèa'}]\n",
      "[{'score': 0.3845702, 'doc': 'B·∫≠t R ƒë·ªãnh lao v√†o ch√©m team b·∫°n v√† th·∫•y tk Fidd b√™n kia'}]\n",
      "[{'score': 0.12234794, 'doc': 'ƒë√¢y ƒë√¢y... üòÅüòÅ'}]\n",
      "[{'score': 0.42590135, 'doc': 'Ko c√≥ ti·ªÅn ad'}]\n",
      "[{'score': 0.26719046, 'doc': 'Ad l√†m v·ªÅ fiora ƒëi , t·ªëc ƒë·ªô ph√° 4 ƒëi·ªÉm y·∫øu , ƒë·ª° chi√™u b·∫±ng w'}]\n",
      "[{'score': 0.33662647, 'doc': 'Nguy·ªÖn Th√†nh Th√°i ƒë·∫•y nh√© B∆∞ c√≤m :3'}]\n",
      "[{'score': 0.20145948, 'doc': 'Nguy·ªÖn Long Vi·ªát Long cbi 1 dan choi xuat hien'}]\n",
      "[{'score': 0.3532365, 'doc': 'S∆∞·ª£t qua? N√™n d·ªãch th√†nh \"Tr∆∞·ª£t t·ªìi, l√™u l√™u!\" ƒê·∫£m b·∫£o team kia ƒë·ªìng lo·∫°t ƒëi mua m√°y m·ªõi... :v'}]\n",
      "[{'score': 0.44042593, 'doc': 'Nguy·ªÖn Minh ƒê·∫°t m√†y c·ªông cho a m·∫•y con t∆∞·ªõng hay ch∆°i ƒëi e h√¥m n√†o a test th·ªÉ l·ª±c üôÇ'}]\n",
      "[{'score': 0.47491366, 'doc': 'Sao AI c·ªßa game n√≥ bi·∫øt l√† d√πng Q n√© skill ta'}]\n",
      "[{'score': 0.30903324, 'doc': 'Con l·∫°y m·∫•y b·ªë tr√°nh xa con ƒë√≥ ra gi√πm con'}]\n",
      "[{'score': 0.45055568, 'doc': 'Tao m√† th·∫•y yasuo l√† tao c·∫ßm b·ªç c√†o v√†o camp ch·∫øt lu√¥n ch·ª© c√†y :))‚Üí_‚Üí'}]\n",
      "[{'score': 0.42413175, 'doc': 'üî• Chi ti·∫øt v·ªÅ Th·∫ßn H·ªèa: http://bit.ly/ThanHoa\\nüî• C√¢u h·ªèi th∆∞·ªùng g·∫∑p: http://bit.ly/ThanHoaFAQ\\nüî• Tra c·ª©u Th·∫ßn Ho·∫£: https://thanhoa.lienminh.garena.vn/tra-cuu/'}]\n",
      "[{'score': 0.6228381, 'doc': 'Li√™n Minh Huy·ªÅn Tho·∫°i cho e h·ªèi v√≠ d·ª• h m√¨nh mua full b·ªô r·ªìi l√∫c ra t∆∞·ªõng m·ªõi c√≥ ph·∫£i mua n·ªØa ko hay l√† c√≥ lu√¥n ·∫°'}]\n",
      "[{'score': 0.45743617, 'doc': ':)) ƒë√£ t·∫Øt cmn th·∫ßn ho·∫£ ƒëi, nh√¨n ch·∫£ ra g√¨ c√≤n c·ª© hi·ªán l√™n r·ªëi m·∫Øt vl'}]\n",
      "[{'score': 0.48172066, 'doc': 'Mua c√°i n√†y c√≥ ƒë√°ng k nh·ªÉ'}]\n",
      "[{'score': 0.5227518, 'doc': 'Ph·∫£i c√≥ Th·∫ßn Ho·∫£ ‚ÄúI was in Alpha‚Äù s·ªë l·∫ßn ch·∫øt trong Alpha m·ªõi hay :v'}]\n",
      "[{'score': 0.33375838, 'doc': 'r·ªìi m·ª•c Clash ƒë√¢u ·∫°'}]\n",
      "[{'score': 0.19873618, 'doc': 'Thresh ƒëi ad ∆°i :>'}]\n",
      "[{'score': 0.33456948, 'doc': 'Li√™n Minh Huy·ªÅn Tho·∫°i ∆°n ·∫°'}]\n",
      "[{'score': 0.55800045, 'doc': 'Nguy·ªÖn Xu√¢n Ki√™n s·∫Ω c√≥ lu√¥n c√°c t∆∞·ªõng m·ªõi sau n√†y nh√© üòâ b·∫£o h√†nh v√† c·∫≠p nh·∫≠t tr·ªçn ƒë·ªùi'}]\n",
      "[{'score': 0.18882833, 'doc': 'Ho√†ng H·∫£i'}]\n",
      "[{'score': 0.43982422, 'doc': 'Th·∫ßn ho·∫£ c·ªßa lux kh√≥ qu√° m·∫•y m√° ∆°i, to√†n m≈© ph√π thu·ª∑ m√† n√≥ k√™u ch·∫∑n st b·∫±ng W n√® ü§¶\\u200d‚ôÄÔ∏è'}]\n",
      "[{'score': 0.29664302, 'doc': 'ƒë√°nh m√°y c√≥ t√≠nh k z :))'}]\n",
      "[{'score': 0.38191056, 'doc': 'T·ªëi c·ªï qu√° ch·∫£ hi·ªÉu mn n√≥i g√¨ lu√¥n @@ ho·∫£ th·∫ßn g√¨ ·∫•y'}]\n"
     ]
    }
   ],
   "source": [
    "for i in data:\n",
    "    print(ds.calculate_similarity(post,i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.40205836, 'doc': 'Mua c√°i n√†y ƒëc cc g√¨ v·∫≠y ae ?'}]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyvi import ViTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv(\"d_comment.csv\")[\"content (S)\"].values\n",
    "post=pd.read_csv(\"d_post.csv\")[\"content (S)\"].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['th·∫•y',\n",
       " 'nhi·ªÅu',\n",
       " 'anh_em',\n",
       " 'quan_t√¢m',\n",
       " 'vi·ªác',\n",
       " 'th√¥ng_th·∫°o',\n",
       " 'n√¢ng_c·∫•p',\n",
       " 'v·∫°ch',\n",
       " 'v·∫°ch',\n",
       " 'v·ªõi',\n",
       " 'th·∫ßn',\n",
       " 'ho·∫£',\n",
       " 'n√™n',\n",
       " 'admin',\n",
       " 'gi·∫£i_ƒë√°p_ƒë√¢y',\n",
       " 'ctrl',\n",
       " 'c√≥',\n",
       " 'th√¥ng_th·∫°o',\n",
       " 'l√†',\n",
       " 'n√¢ng_c·∫•p',\n",
       " 'ƒë∆∞·ª£c',\n",
       " 'm·ªü',\n",
       " 'kho√°',\n",
       " 'b·ªô',\n",
       " 'th·∫ßn',\n",
       " 'ho·∫£',\n",
       " 'th√¨',\n",
       " 'ƒë∆∞·ª£c',\n",
       " 'v·∫°ch',\n",
       " 'hi·ªán',\n",
       " 'm·ªõi',\n",
       " 'c√≥',\n",
       " 'b·ªô',\n",
       " 'n√™n',\n",
       " 'ƒë∆∞·ª£c',\n",
       " 't·ªëi_ƒëa',\n",
       " 'v·∫°ch',\n",
       " 'xem']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ViTokenizer.tokenize(clean_str(post)).split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "my_vector=np.array((word2vec_model[\"c√≥\"]+word2vec_model[\"ƒë·∫πp\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_model[\"c√≥\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model.relative_cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('kh√¥ng', 0.6991002559661865),\n",
       " ('ƒë·∫πp', 0.6972807049751282),\n",
       " ('ch·∫≥ng', 0.6566832661628723),\n",
       " ('ko', 0.5434634685516357),\n",
       " ('ƒëoan_trang', 0.5099766850471497),\n",
       " ('ƒë·∫πp_ƒë·∫Ω', 0.49741852283477783),\n",
       " ('ki√™u_k√¨', 0.488324910402298),\n",
       " ('k√™nh_ki·ªáu', 0.48808908462524414),\n",
       " ('ch·∫£', 0.48420581221580505),\n",
       " ('ki√™u_k·ª≥', 0.4794056713581085)]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_model.similar_by_vector(my_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ti√™u_c·ª±c', 0.4931569993495941),\n",
       " ('x·∫•u_t√≠nh', 0.44311946630477905),\n",
       " ('x·∫•u_xa', 0.43325909972190857),\n",
       " ('b·∫•t_l·ª£i', 0.42398881912231445),\n",
       " ('m√©o_m√≥', 0.4196334481239319),\n",
       " ('t·ªìi_t·ªá', 0.4194181263446808),\n",
       " ('t·ªët', 0.402926504611969),\n",
       " ('y·∫øu', 0.4019845128059387),\n",
       " ('nguy_h·∫°i', 0.39621004462242126),\n",
       " ('x·∫•u_x√≠', 0.3851279616355896)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_model.similar_by_word(\"x·∫•u\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('to√†n_c·∫ßu_khi·∫øn', 0.3115185797214508),\n",
       " ('bao_g·ªìm', 0.2992875277996063),\n",
       " ('dfi', 0.28283464908599854),\n",
       " ('l∆∞∆°ng_th·ª±c_t·∫°i', 0.27744320034980774),\n",
       " ('trung_t√¢m_b√£o', 0.27666351199150085),\n",
       " ('g·ªìm', 0.27442577481269836),\n",
       " ('csf', 0.2708356976509094),\n",
       " ('krasukha', 0.2690920829772949),\n",
       " ('t·∫≠p_ƒëo√†n', 0.26735156774520874),\n",
       " ('jamshedpur', 0.26627618074417114)]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_model.most_similar(negative=['kh√¥ng', 'vui'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('g√°i', 0.6967887878417969)]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_model.most_similar(positive=['trai', 'v·ª£'], negative=['ch·ªìng'],topn=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from CocCocTokenizer import PyTokenizer\n",
    "T = PyTokenizer(load_nontone_data=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "matrix_vec_train=[]\n",
    "for i in x_train:\n",
    "    matrix_vec_train.append(makeFeatureVec(T.word_tokenize(i, tokenize_option=0),word2vec_model,100).reshape(100*300))\n",
    "matrix_vec_train=np.array(matrix_vec_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_vec_test=[]\n",
    "for i in x_test:\n",
    "    matrix_vec_test.append(makeFeatureVec(T.word_tokenize(i, tokenize_option=0),word2vec_model,300).reshape(100*300))\n",
    "matrix_vec_test=np.array(matrix_vec_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = max(len(x) for x in matrix_vec_train) # longest text in train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = matrix_vec_train.shape[1]\n",
    "filter_sizes = [3,4,5]\n",
    "num_filters = 100\n",
    "drop = 0.5\n",
    "EMBEDDING_DIM=300\n",
    "inputs = Input(shape=(sequence_length,))\n",
    "embedding_layer = Embedding(len(word2vec_model.vocab),\n",
    "                           EMBEDDING_DIM,\n",
    "                           weights=[word2vec_model.vectors])\n",
    "embedding = embedding_layer(inputs)\n",
    "reshape = Reshape((sequence_length,EMBEDDING_DIM,1))(embedding)\n",
    "\n",
    "conv_0 = Conv2D(num_filters, (filter_sizes[0], EMBEDDING_DIM),activation='relu',kernel_regularizer=regularizers.l2(0.01))(reshape)\n",
    "conv_1 = Conv2D(num_filters, (filter_sizes[1], EMBEDDING_DIM),activation='relu',kernel_regularizer=regularizers.l2(0.01))(reshape)\n",
    "conv_2 = Conv2D(num_filters, (filter_sizes[2], EMBEDDING_DIM),activation='relu',kernel_regularizer=regularizers.l2(0.01))(reshape)\n",
    "\n",
    "maxpool_0 = MaxPooling2D((sequence_length - filter_sizes[0] + 1, 1), strides=(1,1))(conv_0)\n",
    "maxpool_1 = MaxPooling2D((sequence_length - filter_sizes[1] + 1, 1), strides=(1,1))(conv_1)\n",
    "maxpool_2 = MaxPooling2D((sequence_length - filter_sizes[2] + 1, 1), strides=(1,1))(conv_2)\n",
    "\n",
    "merged_tensor = concatenate([maxpool_0, maxpool_1, maxpool_2], axis=1)\n",
    "flatten = Flatten()(merged_tensor)\n",
    "reshape = Reshape((3*num_filters,))(flatten)\n",
    "dropout = Dropout(drop)(flatten)\n",
    "output = Dense(units=2, activation='softmax',kernel_regularizer=regularizers.l2(0.01))(dropout)\n",
    "model = Model(inputs, output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 30000)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 30000, 300)   131716800   input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 30000, 300, 1 0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 29998, 1, 100 90100       reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 29997, 1, 100 120100      reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 29996, 1, 100 150100      reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 1, 1, 100)    0           conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 1, 1, 100)    0           conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 1, 1, 100)    0           conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 3, 1, 100)    0           max_pooling2d_1[0][0]            \n",
      "                                                                 max_pooling2d_2[0][0]            \n",
      "                                                                 max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 300)          0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 300)          0           flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 2)            602         dropout_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 132,077,702\n",
      "Trainable params: 132,077,702\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X train and X validation tensor: (1000, 30000) (1000, 30000)\n",
      "Shape of label train and validation tensor: (1000, 2) (500, 2)\n"
     ]
    }
   ],
   "source": [
    "print('Shape of X train and X validation tensor:', matrix_vec_train.shape,matrix_vec_test.shape)\n",
    "print('Shape of label train and validation tensor:', y_train.shape,y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.24589975,  0.77774334, -1.2496276 , ...,  0.        ,\n",
       "          0.        ,  0.        ]],\n",
       "\n",
       "       [[ 0.7005746 ,  1.3664861 , -1.6598337 , ...,  0.        ,\n",
       "          0.        ,  0.        ]],\n",
       "\n",
       "       [[-0.01931669,  1.5146191 , -0.95038223, ...,  0.        ,\n",
       "          0.        ,  0.        ]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 1.5678059 ,  0.9919252 , -1.4614412 , ...,  0.        ,\n",
       "          0.        ,  0.        ]],\n",
       "\n",
       "       [[-0.01931669,  1.5146191 , -0.95038223, ...,  0.        ,\n",
       "          0.        ,  0.        ]],\n",
       "\n",
       "       [[-0.79908675,  1.0495341 , -0.29474068, ...,  0.        ,\n",
       "          0.        ,  0.        ]]], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.expand_dims(matrix_vec_train, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(matrix_vec_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/coccoc_word_tokenize/lib/python3.6/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": " indices[1,3] = -1 is not in [0, 439056)\n\t [[node embedding_1/embedding_lookup (defined at /opt/anaconda3/envs/coccoc_word_tokenize/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py:1751) ]] [Op:__inference_keras_scratch_graph_2385]\n\nFunction call stack:\nkeras_scratch_graph\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-0ee58cb0b9d1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mEarlyStopping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m model.fit(matrix_vec_train, y_train, batch_size=10, epochs=10, verbose=1,\n\u001b[0;32m----> 8\u001b[0;31m          callbacks=callbacks) \n\u001b[0m",
      "\u001b[0;32m/opt/anaconda3/envs/coccoc_word_tokenize/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1239\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m     def evaluate(self,\n",
      "\u001b[0;32m/opt/anaconda3/envs/coccoc_word_tokenize/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    194\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/coccoc_word_tokenize/lib/python3.6/site-packages/tensorflow_core/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3738\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3739\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3740\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3741\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3742\u001b[0m     \u001b[0;31m# EagerTensor.numpy() will often make a copy to ensure memory safety.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/coccoc_word_tokenize/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1079\u001b[0m       \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mFor\u001b[0m \u001b[0minvalid\u001b[0m \u001b[0mpositional\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mkeyword\u001b[0m \u001b[0margument\u001b[0m \u001b[0mcombinations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1080\u001b[0m     \"\"\"\n\u001b[0;32m-> 1081\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1082\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1083\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/coccoc_word_tokenize/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1119\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[1;32m   1120\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[0;32m-> 1121\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/coccoc_word_tokenize/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1222\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1223\u001b[0m       flat_outputs = forward_function.call(\n\u001b[0;32m-> 1224\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1225\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1226\u001b[0m       \u001b[0mgradient_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delayed_rewrite_functions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/coccoc_word_tokenize/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    509\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    512\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m/opt/anaconda3/envs/coccoc_word_tokenize/lib/python3.6/site-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m     \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     keras_symbolic_tensors = [\n",
      "\u001b[0;32m/opt/anaconda3/envs/coccoc_word_tokenize/lib/python3.6/site-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m:  indices[1,3] = -1 is not in [0, 439056)\n\t [[node embedding_1/embedding_lookup (defined at /opt/anaconda3/envs/coccoc_word_tokenize/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py:1751) ]] [Op:__inference_keras_scratch_graph_2385]\n\nFunction call stack:\nkeras_scratch_graph\n"
     ]
    }
   ],
   "source": [
    "adam = Adam(lr=1e-3)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=adam,\n",
    "              metrics=['acc'])\n",
    "callbacks = [EarlyStopping(monitor='val_loss')]\n",
    "model.fit(matrix_vec_train, y_train, batch_size=10, epochs=10, verbose=1, validation_data=(matrix_vec_test, y_test),\n",
    "         callbacks=callbacks) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 2)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define CNN architecture\n",
    "# del model\n",
    "embedding_dim = 100\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.Embedding(len(word2vec_model.vocab),\n",
    "                           word2vec_model.vectors.shape[1],\n",
    "                           weights=[word2vec_model.vectors],\n",
    "                           input_length=maxlen))\n",
    "model.add(layers.Conv2D(100, kernel_size = (3, 300), padding='valid', data_format=\"channels_last\", input_shape = (1000, 30000), activation='relu',kernel_regularizer=regularizers.l2(0.01)))\n",
    "model.add(layers.GlobalMaxPooling1D())\n",
    "model.add(layers.Dense(10, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit model\n",
    "history = model.fit(matrix_vec_train, y_train,\n",
    "                    epochs=5,\n",
    "                    verbose=True,\n",
    "                    validation_data=(matrix_vec_test, y_test),\n",
    "                    batch_size=64)\n",
    "loss, accuracy = model.evaluate(matrix_vec_train, y_train, verbose=True)\n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "loss, accuracy = model.evaluate(matrix_vec_test, y_test, verbose=False)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
